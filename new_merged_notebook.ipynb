{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "telemetry = pd.read_csv('dataset/PdM_telemetry.csv')\n",
    "errors = pd.read_csv('dataset/PdM_errors.csv')\n",
    "maint = pd.read_csv('dataset/PdM_maint.csv')\n",
    "failures = pd.read_csv('dataset/PdM_failures.csv')\n",
    "machines = pd.read_csv('dataset/PdM_machines.csv')\n",
    "\n",
    "# format datetime field which comes in as string\n",
    "telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(\"Total number of telemetry records: %d\" % len(telemetry.index))\n",
    "print(telemetry.head())\n",
    "telemetry.describe()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plot_df = telemetry.loc[(telemetry['machineID'] == 1) & \n",
    "                        (telemetry['datetime'] > pd.to_datetime('2015-01-01')) & \n",
    "                        (telemetry['datetime'] <pd.to_datetime('2015-02-01')),\n",
    "                        ['datetime','volt']]\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(plot_df['datetime'], plot_df['volt'])\n",
    "plt.ylabel('voltage')\n",
    "\n",
    "# make x-axis ticks legible\n",
    "adf = plt.gca().get_xaxis().get_major_formatter()\n",
    "adf.scaled[1.0] = '%m-%d-%Y'\n",
    "plt.xlabel('Date')\n",
    "\n",
    "# format of datetime field which comes in as string\n",
    "errors['datetime'] = pd.to_datetime(errors['datetime'],format = '%Y-%m-%d %H:%M:%S')\n",
    "errors['errorID'] = errors['errorID'].astype('category')\n",
    "print(\"Total Number of error records: %d\" %len(errors.index))\n",
    "errors.head()\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(20, 8))\n",
    "errors['errorID'].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')\n",
    "errors['errorID'].value_counts()\n",
    "\n",
    "maint['datetime'] = pd.to_datetime(maint['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "maint['comp'] = maint['comp'].astype('category')\n",
    "print(\"Total Number of maintenance Records: %d\" %len(maint.index))\n",
    "maint.head()\n",
    "\n",
    "machines['model'] = machines['model'].astype('category')\n",
    "\n",
    "print(\"Total number of machines: %d\" % len(machines.index))\n",
    "machines.head()\n",
    "\n",
    "# format datetime field which comes in as string\n",
    "failures['datetime'] = pd.to_datetime(failures['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "failures['failure'] = failures['failure'].astype('category')\n",
    "\n",
    "print(\"Total number of failures: %d\" % len(failures.index))\n",
    "failures.head()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate mean values for telemetry features\n",
    "fields = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "telemetry_mean_3h = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "for col in fields:\n",
    "    # Pivot the telemetry DataFrame and calculate mean values resampled every 3 hours\n",
    "    temp = pd.pivot_table(telemetry,\n",
    "                          index='datetime',\n",
    "                          columns='machineID',\n",
    "                          values=col).resample('3H', closed='left', label='right').mean().unstack()\n",
    "    telemetry_mean_3h[col + 'mean_3h'] = temp  # Add mean values to the telemetry_mean_3h DataFrame\n",
    "\n",
    "telemetry_mean_3h.reset_index(inplace=True)  # Reset index\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "telemetry_mean_3h.head()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate mean values for telemetry features\n",
    "fields = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "telemetry_mean_3h = pd.DataFrame()  # Initialize an empty DataFrame for mean values\n",
    "telemetry_sd_3h = pd.DataFrame()    # Initialize an empty DataFrame for standard deviation values\n",
    "\n",
    "for col in fields:\n",
    "    # Pivot the telemetry DataFrame and calculate mean values resampled every 3 hours\n",
    "    temp_mean = pd.pivot_table(telemetry,\n",
    "                               index='datetime',\n",
    "                               columns='machineID',\n",
    "                               values=col).resample('3H', closed='left', label='right').mean().unstack()\n",
    "    # Add mean values to the telemetry_mean_3h DataFrame\n",
    "    telemetry_mean_3h[col + 'mean_3h'] = temp_mean\n",
    "    \n",
    "    # Pivot the telemetry DataFrame and calculate standard deviation values resampled every 3 hours\n",
    "    temp_std = pd.pivot_table(telemetry,\n",
    "                              index='datetime',\n",
    "                              columns='machineID',\n",
    "                              values=col).resample('3H', closed='left', label='right').std().unstack()\n",
    "    # Add standard deviation values to the telemetry_sd_3h DataFrame\n",
    "    telemetry_sd_3h[col + 'sd_3h'] = temp_std\n",
    "\n",
    "# Reset index for both DataFrames\n",
    "telemetry_mean_3h.reset_index(inplace=True)\n",
    "telemetry_sd_3h.reset_index(inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame for mean values\n",
    "telemetry_mean_3h.head()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate rolling mean values for telemetry features\n",
    "fields = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "telemetry_mean_24h = pd.DataFrame()  # Initialize an empty DataFrame for rolling mean values\n",
    "telemetry_sd_24h = pd.DataFrame()    # Initialize an empty DataFrame for rolling standard deviation values\n",
    "\n",
    "for col in fields:\n",
    "    # Calculate rolling mean values with a window of 24 hours\n",
    "    temp_mean = pd.pivot_table(telemetry,\n",
    "                               index='datetime',\n",
    "                               columns='machineID',\n",
    "                               values=col).rolling(window=24).mean().resample('3H', closed='left', label='right').first().unstack()\n",
    "    # Add rolling mean values to the telemetry_mean_24h DataFrame\n",
    "    telemetry_mean_24h[col + 'mean_24h'] = temp_mean\n",
    "\n",
    "    # Calculate rolling standard deviation values with a window of 24 hours\n",
    "    temp_std = pd.pivot_table(telemetry,\n",
    "                              index='datetime',\n",
    "                              columns='machineID',\n",
    "                              values=col).rolling(window=24).std().resample('3H', closed='left', label='right').first().unstack()\n",
    "    # Add rolling standard deviation values to the telemetry_sd_24h DataFrame\n",
    "    telemetry_sd_24h[col + 'sd_24h'] = temp_std\n",
    "\n",
    "# Drop rows with NaN values for 'voltmean_24h' column\n",
    "telemetry_mean_24h = telemetry_mean_24h.dropna(subset=['voltmean_24h'])\n",
    "\n",
    "# Drop rows with NaN values for 'voltsd_24h' column\n",
    "telemetry_sd_24h = telemetry_sd_24h.dropna(subset=['voltsd_24h'])\n",
    "\n",
    "# Reset index for both DataFrames\n",
    "telemetry_mean_24h.reset_index(inplace=True)\n",
    "telemetry_sd_24h.reset_index(inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame for rolling mean values\n",
    "telemetry_mean_24h.head()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate and merge columns from feature sets\n",
    "telemetry_feat = pd.concat([telemetry_mean_3h,\n",
    "                            telemetry_sd_3h.iloc[:, 2:6],\n",
    "                            telemetry_mean_24h.iloc[:, 2:6],\n",
    "                            telemetry_sd_24h.iloc[:, 2:6]], axis=1).dropna()\n",
    "\n",
    "# Display summary statistics of the merged DataFrame\n",
    "telemetry_feat.describe()\n",
    "\n",
    "\n",
    "errors\n",
    "\n",
    "# create a column for each error type\n",
    "error_count = pd.get_dummies(errors.set_index('datetime')).reset_index()\n",
    "error_count\n",
    "error_count.columns = ['datetime', 'machineID', 'error1', 'error2', 'error3', 'error4', 'error5']\n",
    "error_count.head(13)\n",
    "\n",
    "# create a column for each error type\n",
    "error_count = pd.get_dummies(errors.set_index('datetime')).reset_index()\n",
    "error_count\n",
    "error_count.columns = ['datetime', 'machineID', 'error1', 'error2', 'error3', 'error4', 'error5']\n",
    "error_count.head(13)\n",
    "\n",
    "# combine errors for a given machine in a given hour\n",
    "error_count = error_count.groupby(['machineID','datetime']).sum().reset_index()\n",
    "error_count.head(13)\n",
    "\n",
    "error_count = telemetry[['datetime', 'machineID']].merge(error_count, on=['machineID', 'datetime'], how='left').fillna(0.0)\n",
    "error_count.describe()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define fields\n",
    "fields = ['error%d' % i for i in range(1, 6)]\n",
    "\n",
    "# Initialize a list to store temporary results\n",
    "temp = []\n",
    "\n",
    "# Iterate over each error column\n",
    "for col in fields:\n",
    "    # Calculate rolling sum with a window of 24 hours, then resample to 3-hour intervals\n",
    "    temp.append(pd.pivot_table(error_count,\n",
    "                               index='datetime',\n",
    "                               columns='machineID',\n",
    "                               values=col)\n",
    "                .rolling(window=24)\n",
    "                .sum()\n",
    "                .resample('3H', closed='left', label='right')\n",
    "                .first()\n",
    "                .unstack())\n",
    "\n",
    "# Concatenate the temporary results along the column axis\n",
    "error_count = pd.concat(temp, axis=1)\n",
    "\n",
    "# Rename columns\n",
    "error_count.columns = [i + 'count' for i in fields]\n",
    "\n",
    "# Reset index\n",
    "error_count.reset_index(inplace=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "error_count = error_count.dropna()\n",
    "\n",
    "# Display summary statistics\n",
    "error_count.describe()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# create a column for each error type\n",
    "comp_rep = pd.get_dummies(maint.set_index('datetime')).reset_index()\n",
    "comp_rep.columns = ['datetime', 'machineID', 'comp1', 'comp2', 'comp3', 'comp4']\n",
    "\n",
    "# combine repairs for a given machine in a given hour\n",
    "comp_rep = comp_rep.groupby(['machineID', 'datetime']).sum().reset_index()\n",
    "\n",
    "# add timepoints where no components were replaced\n",
    "comp_rep = telemetry[['datetime', 'machineID']].merge(comp_rep,\n",
    "                                                      on=['datetime', 'machineID'],\n",
    "                                                      how='outer').fillna(0).sort_values(by=['machineID', 'datetime'])\n",
    "\n",
    "components = ['comp1', 'comp2', 'comp3', 'comp4']\n",
    "for comp in components:\n",
    "    # convert indicator to most recent date of component change\n",
    "    comp_rep.loc[comp_rep[comp] < 1, comp] = None\n",
    "    comp_rep.loc[-comp_rep[comp].isnull(), comp] = comp_rep.loc[-comp_rep[comp].isnull(), 'datetime']\n",
    "    \n",
    "    # forward-fill the most-recent date of component change\n",
    "    comp_rep[comp] = comp_rep[comp].fillna(method='ffill')\n",
    "\n",
    "# remove dates in 2014 (may have NaN or future component change dates)    \n",
    "comp_rep = comp_rep.loc[comp_rep['datetime'] > pd.to_datetime('2015-01-01')]\n",
    "\n",
    "# replace dates of most recent component change with days since most recent component change\n",
    "for comp in components:\n",
    "    comp_rep[comp] = (comp_rep['datetime'] - comp_rep[comp]) / np.timedelta64(1, 'D')\n",
    "    \n",
    "comp_rep.describe()\n",
    "\n",
    "telemetry_feat\n",
    "\n",
    "final_feat = telemetry_feat.merge(error_count, on=['datetime', 'machineID'], how='left')\n",
    "final_feat = final_feat.merge(comp_rep, on=['datetime', 'machineID'], how='left')\n",
    "final_feat = final_feat.merge(machines, on=['machineID'], how='left')\n",
    "\n",
    "print(final_feat.head())\n",
    "final_feat.describe()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming final_feat is prepared with all relevant features without labels\n",
    "\n",
    "# Preprocessing: handle missing values\n",
    "final_feat.fillna(method='bfill', inplace=True)  # Fill missing values using backward fill method\n",
    "final_feat.fillna(method='ffill', inplace=True)  # Fill remaining missing values using forward fill method\n",
    "\n",
    "# Drop non-numeric columns\n",
    "final_feat_numeric = final_feat.select_dtypes(include='number')\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "final_feat_normalized = scaler.fit_transform(final_feat_numeric)\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow method\n",
    "import matplotlib.pyplot as plt\n",
    "wcss = []  # Within-cluster sum of squares\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(final_feat_normalized)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# From the Elbow Method plot, choose the optimal number of clusters\n",
    "\n",
    "# Apply KMeans clustering\n",
    "num_clusters = 3  # Choose the optimal number of clusters based on the Elbow Method\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "final_feat['cluster'] = kmeans.fit_predict(final_feat_normalized)\n",
    "\n",
    "# Analyze clusters to identify patterns indicative of potential failure\n",
    "cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=final_feat_numeric.columns)\n",
    "print(cluster_centers)\n",
    "\n",
    "# You can further analyze the clusters to identify patterns that might indicate potential failures\n",
    "\n",
    "\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Davies-Bouldin index\n",
    "db_index = davies_bouldin_score(final_feat_normalized, final_feat['cluster'])\n",
    "print(\"Davies-Bouldin Index:\", db_index)\n",
    "\n",
    "# Calinski-Harabasz index\n",
    "ch_index = calinski_harabasz_score(final_feat_normalized, final_feat['cluster'])\n",
    "print(\"Calinski-Harabasz Index:\", ch_index)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "final_feat_pca = pca.fit_transform(final_feat_normalized)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_feat_pca[:, 0], final_feat_pca[:, 1], c=final_feat['cluster'], cmap='viridis', alpha=0.5)\n",
    "plt.title('Clusters Visualization (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Transform the cluster centers to the PCA space\n",
    "cluster_centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_feat_pca[:, 0], final_feat_pca[:, 1], c=final_feat['cluster'], cmap='viridis', alpha=0.5)\n",
    "plt.scatter(cluster_centers_pca[:, 0], cluster_centers_pca[:, 1], marker='x', s=100, c='red', label='Cluster Centers')\n",
    "plt.title('Clusters with Cluster Centers (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Count the number of data points in each cluster\n",
    "cluster_counts = final_feat['cluster'].value_counts().sort_index()\n",
    "\n",
    "# Plot cluster size distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "cluster_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Cluster Size Distribution')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Data Points')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the columns to identify suitable features\n",
    "print(final_feat.columns)\n",
    "\n",
    "# Select a subset of real feature names for pairwise plots\n",
    "selected_features = ['voltmean_3h', 'rotatemean_3h', 'pressuremean_3h']  # Replace with actual feature names\n",
    "\n",
    "# Pairwise plot\n",
    "sns.pairplot(final_feat[selected_features + ['cluster']], hue='cluster', palette='viridis')\n",
    "plt.suptitle('Pairwise Feature Plots', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming final_feat is already prepared with all relevant features without labels\n",
    "\n",
    "# Preprocessing: handle missing values\n",
    "final_feat.fillna(method='bfill', inplace=True)  # Fill missing values using backward fill method\n",
    "final_feat.fillna(method='ffill', inplace=True)  # Fill remaining missing values using forward fill method\n",
    "\n",
    "# Drop non-numeric columns\n",
    "final_feat_numeric = final_feat.select_dtypes(include='number')\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "final_feat_normalized = scaler.fit_transform(final_feat_numeric)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)  # Adjusted parameters\n",
    "final_feat['cluster'] = dbscan.fit_predict(final_feat_normalized)\n",
    "\n",
    "# Number of clusters (excluding noise)\n",
    "num_clusters = len(set(final_feat['cluster'])) - (1 if -1 in final_feat['cluster'] else 0)\n",
    "print(\"Number of clusters:\", num_clusters)\n",
    "\n",
    "# Number of noise points\n",
    "num_noise = list(final_feat['cluster']).count(-1)\n",
    "print(\"Number of noise points:\", num_noise)\n",
    "\n",
    "# Visualize clusters using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "final_feat_pca = pca.fit_transform(final_feat_normalized)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=final_feat_pca[:, 0], y=final_feat_pca[:, 1], hue=final_feat['cluster'], palette='viridis', alpha=0.5)\n",
    "plt.title('DBSCAN Clusters Visualization (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming final_feat is already prepared with all relevant features without labels\n",
    "\n",
    "# Preprocessing: handle missing values\n",
    "final_feat.fillna(method='bfill', inplace=True)  # Fill missing values using backward fill method\n",
    "final_feat.fillna(method='ffill', inplace=True)  # Fill remaining missing values using forward fill method\n",
    "\n",
    "# Drop non-numeric columns\n",
    "final_feat_numeric = final_feat.select_dtypes(include='number')\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "final_feat_normalized = scaler.fit_transform(final_feat_numeric)\n",
    "\n",
    "# Apply Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)  # You can choose the number of components\n",
    "final_feat['cluster'] = gmm.fit_predict(final_feat_normalized)\n",
    "\n",
    "# Visualize clusters using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "final_feat_pca = pca.fit_transform(final_feat_normalized)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=final_feat_pca[:, 0], y=final_feat_pca[:, 1], hue=final_feat['cluster'], palette='viridis', alpha=0.5)\n",
    "plt.title('Gaussian Mixture Model Clusters Visualization (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming final_feat is already prepared with all relevant features without labels\n",
    "\n",
    "# Preprocessing: handle missing values\n",
    "final_feat.fillna(method='bfill', inplace=True)  # Fill missing values using backward fill method\n",
    "final_feat.fillna(method='ffill', inplace=True)  # Fill remaining missing values using forward fill method\n",
    "\n",
    "# Drop non-numeric columns\n",
    "final_feat_numeric = final_feat.select_dtypes(include='number')\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "final_feat_normalized = scaler.fit_transform(final_feat_numeric)\n",
    "\n",
    "\n",
    "# Apply Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)  # You can choose the number of components\n",
    "final_feat['cluster'] = gmm.fit_predict(final_feat_normalized)\n",
    "\n",
    "\n",
    "# Visualize clusters using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "final_feat_pca = pca.fit_transform(final_feat_normalized)\n",
    "\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=final_feat_pca[:, 0], y=final_feat_pca[:, 1], hue=final_feat['cluster'], palette='viridis', alpha=0.5)\n",
    "plt.title('Gaussian Mixture Model Clusters Visualization (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
